{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f39ea51",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3fc30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def fuzzy_match(preprocessed_text, preprocessed_cpt_df, threshold=70):\n",
    "    \"\"\"\n",
    "    Perform fuzzy matching between preprocessed text and CPT descriptions.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with preprocessed CPT descriptions.\n",
    "        threshold (int): Minimum score for fuzzy matching (0-100).\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and descriptions.\n",
    "    \"\"\"\n",
    "    matched_results = []\n",
    "    \n",
    "    for text in preprocessed_text:\n",
    "        # Find the best match using fuzzy matching for each text\n",
    "        best_match, score = process.extractOne(text, preprocessed_cpt_df['Description'])\n",
    "        \n",
    "        # Check if the score meets the threshold\n",
    "        if score >= threshold:\n",
    "            # Find corresponding CPT code\n",
    "            matched_row = preprocessed_cpt_df[preprocessed_cpt_df['Description'] == best_match]\n",
    "            cpt_code = matched_row['Code'].values[0]\n",
    "            matched_results.append((cpt_code, best_match, score))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "fuzzy_matches = fuzzy_match(processed_text, processed_cpt_df)\n",
    "for match in fuzzy_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Score: {match[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe4ecb",
   "metadata": {},
   "source": [
    "## Synonym-Based Matching (Word Embeddings with Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02847098",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assume preprocessed_text and preprocessed_cpt_df are tokenized\n",
    "def synonym_match(preprocessed_text, preprocessed_cpt_df):\n",
    "    \"\"\"\n",
    "    Perform synonym-based matching using Word2Vec embeddings.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed tokenized text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with tokenized CPT descriptions.\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and descriptions.\n",
    "    \"\"\"\n",
    "    # Combine all text data (pitch deck + CPT descriptions) to train Word2Vec model\n",
    "    combined_data = preprocessed_text + preprocessed_cpt_df['Description'].apply(word_tokenize).tolist()\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(sentences=combined_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    matched_results = []\n",
    "    \n",
    "    for text in preprocessed_text:\n",
    "        # Find the most similar CPT description using Word2Vec similarity\n",
    "        max_similarity = 0\n",
    "        best_match = None\n",
    "        best_code = None\n",
    "        \n",
    "        for _, row in preprocessed_cpt_df.iterrows():\n",
    "            similarity = model.wv.n_similarity(text, word_tokenize(row['Description']))\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = row['Description']\n",
    "                best_code = row['Code']\n",
    "        \n",
    "        if best_match:\n",
    "            matched_results.append((best_code, best_match, max_similarity))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "synonym_matches = synonym_match(processed_text, processed_cpt_df)\n",
    "for match in synonym_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Similarity: {match[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b653a2e",
   "metadata": {},
   "source": [
    "## TF-IDF Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd3fd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def tfidf_match(preprocessed_text, preprocessed_cpt_df):\n",
    "    \"\"\"\n",
    "    Perform matching using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with preprocessed CPT descriptions.\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and descriptions.\n",
    "    \"\"\"\n",
    "    # Combine the text for TF-IDF vectorization\n",
    "    corpus = preprocessed_text + preprocessed_cpt_df['Description'].tolist()\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Split the matrix into pitch deck text and CPT descriptions\n",
    "    text_matrix = tfidf_matrix[:len(preprocessed_text)]\n",
    "    cpt_matrix = tfidf_matrix[len(preprocessed_text):]\n",
    "    \n",
    "    matched_results = []\n",
    "    \n",
    "    for i, text_vector in enumerate(text_matrix):\n",
    "        # Compute cosine similarity between pitch deck text and all CPT descriptions\n",
    "        similarity = cosine_similarity(text_vector, cpt_matrix)\n",
    "        \n",
    "        # Find the best match\n",
    "        best_match_idx = similarity.argmax()\n",
    "        best_match_description = preprocessed_cpt_df.iloc[best_match_idx]['Description']\n",
    "        best_match_code = preprocessed_cpt_df.iloc[best_match_idx]['Code']\n",
    "        best_similarity_score = similarity[0, best_match_idx]\n",
    "        \n",
    "        matched_results.append((best_match_code, best_match_description, best_similarity_score))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "tfidf_matches = tfidf_match(processed_text, processed_cpt_df)\n",
    "for match in tfidf_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Similarity: {match[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36ac61",
   "metadata": {},
   "source": [
    "## Cosine Similarity (With BERT Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100dcd12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def cosine_similarity_bert(preprocessed_text, preprocessed_cpt_df):\n",
    "    \"\"\"\n",
    "    Perform cosine similarity matching using BERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with preprocessed CPT descriptions.\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and descriptions.\n",
    "    \"\"\"\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def get_embeddings(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach()\n",
    "\n",
    "    # Get embeddings for pitch deck text and CPT descriptions\n",
    "    text_embeddings = torch.cat([get_embeddings(text) for text in preprocessed_text])\n",
    "    cpt_embeddings = torch.cat([get_embeddings(desc) for desc in preprocessed_cpt_df['Description']])\n",
    "    \n",
    "    matched_results = []\n",
    "    \n",
    "    for text_embedding in text_embeddings:\n",
    "        # Compute cosine similarity with all CPT descriptions\n",
    "        similarity = torch.nn.functional.cosine_similarity(text_embedding.unsqueeze(0), cpt_embeddings)\n",
    "        \n",
    "        # Find the best match\n",
    "        best_match_idx = similarity.argmax().item()\n",
    "        best_match_description = preprocessed_cpt_df.iloc[best_match_idx]['Description']\n",
    "        best_match_code = preprocessed_cpt_df.iloc[best_match_idx]['Code']\n",
    "        best_similarity_score = similarity[best_match_idx].item()\n",
    "        \n",
    "        matched_results.append((best_match_code, best_match_description, best_similarity_score))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "bert_matches = cosine_similarity_bert(processed_text, processed_cpt_df)\n",
    "for match in bert_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Similarity: {match[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cde30",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f608a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')  # You can use 'en_core_sci_md' for a biomedical model\n",
    "\n",
    "def ner_match(preprocessed_text, preprocessed_cpt_df):\n",
    "    \"\"\"\n",
    "    Perform NER-based matching using spaCy NER to extract medical terms and match with CPT codes.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with preprocessed CPT descriptions.\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and descriptions.\n",
    "    \"\"\"\n",
    "    matched_results = []\n",
    "    \n",
    "    for text in preprocessed_text:\n",
    "        # Process text with spaCy NER\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extract entities related to medical procedures or terms\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in ['MEDICAL_CONDITION', 'PROCEDURE', 'SYMPTOM']]\n",
    "        \n",
    "        # Match extracted entities with CPT descriptions\n",
    "        for entity in entities:\n",
    "            for _, row in preprocessed_cpt_df.iterrows():\n",
    "                if entity in row['Description']:\n",
    "                    matched_results.append((row['Code'], row['Description'], entity))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "ner_matches = ner_match(processed_text, processed_cpt_df)\n",
    "for match in ner_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Matched Entity: {match[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419544e",
   "metadata": {},
   "source": [
    "## Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fc2a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def lda_topic_modeling(preprocessed_text, preprocessed_cpt_df, num_topics=5):\n",
    "    \"\"\"\n",
    "    Perform topic modeling using LDA to match the pitch deck with CPT descriptions based on topics.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with preprocessed CPT descriptions.\n",
    "        num_topics (int): Number of topics to extract.\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and topics.\n",
    "    \"\"\"\n",
    "    # Combine the preprocessed text and CPT descriptions into one corpus\n",
    "    combined_texts = preprocessed_text + preprocessed_cpt_df['Description'].tolist()\n",
    "    \n",
    "    # Tokenize and create dictionary\n",
    "    dictionary = corpora.Dictionary([simple_preprocess(text) for text in combined_texts])\n",
    "    corpus = [dictionary.doc2bow(simple_preprocess(text)) for text in combined_texts]\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100)\n",
    "    \n",
    "    # Get topics for pitch deck text\n",
    "    matched_results = []\n",
    "    for i, text in enumerate(preprocessed_text):\n",
    "        bow = dictionary.doc2bow(simple_preprocess(text))\n",
    "        topics = lda_model.get_document_topics(bow)\n",
    "        \n",
    "        # Find the best matching topic and the corresponding CPT code\n",
    "        best_topic = max(topics, key=lambda x: x[1])\n",
    "        best_topic_words = lda_model.show_topic(best_topic[0])\n",
    "        \n",
    "        for _, row in preprocessed_cpt_df.iterrows():\n",
    "            cpt_bow = dictionary.doc2bow(simple_preprocess(row['Description']))\n",
    "            cpt_topics = lda_model.get_document_topics(cpt_bow)\n",
    "            if any(best_topic[0] == topic_id for topic_id, _ in cpt_topics):\n",
    "                matched_results.append((row['Code'], row['Description'], best_topic_words))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "lda_matches = lda_topic_modeling(processed_text, processed_cpt_df)\n",
    "for match in lda_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Topic Words: {match[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d271e",
   "metadata": {},
   "source": [
    "## BERT-Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee6781",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def bert_match(preprocessed_text, preprocessed_cpt_df):\n",
    "    \"\"\"\n",
    "    Perform BERT-based matching using embeddings and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_text (list): List of preprocessed text from pitch deck.\n",
    "        preprocessed_cpt_df (pd.DataFrame): DataFrame with preprocessed CPT descriptions.\n",
    "    \n",
    "    Returns:\n",
    "        matched_results (list): List of matched CPT codes and descriptions.\n",
    "    \"\"\"\n",
    "    def get_bert_embeddings(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach()\n",
    "\n",
    "    # Get BERT embeddings for pitch deck and CPT descriptions\n",
    "    text_embeddings = torch.cat([get_bert_embeddings(text) for text in preprocessed_text])\n",
    "    cpt_embeddings = torch.cat([get_bert_embeddings(desc) for desc in preprocessed_cpt_df['Description']])\n",
    "    \n",
    "    matched_results = []\n",
    "    \n",
    "    for text_embedding in text_embeddings:\n",
    "        # Compute cosine similarity with all CPT descriptions\n",
    "        similarity = torch.nn.functional.cosine_similarity(text_embedding.unsqueeze(0), cpt_embeddings)\n",
    "        \n",
    "        # Find the best match\n",
    "        best_match_idx = similarity.argmax().item()\n",
    "        best_match_description = preprocessed_cpt_df.iloc[best_match_idx]['Description']\n",
    "        best_match_code = preprocessed_cpt_df.iloc[best_match_idx]['Code']\n",
    "        best_similarity_score = similarity[best_match_idx].item()\n",
    "        \n",
    "        matched_results.append((best_match_code, best_match_description, best_similarity_score))\n",
    "    \n",
    "    return matched_results\n",
    "\n",
    "# Example usage:\n",
    "bert_matches = bert_match(processed_text, processed_cpt_df)\n",
    "for match in bert_matches:\n",
    "    print(f\"CPT Code: {match[0]}, Description: {match[1]}, Similarity: {match[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
